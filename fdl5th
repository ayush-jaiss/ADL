import math
import pandas as pd
import tensorflow as tf
import keras_tuner as kt
import matplotlib.pyplot as plt 
from tensorflow.keras import Model
from tensorflow.keras import Sequential
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras. losses import MeanSquaredLogarithmicError
from sklearn.preprocessing import MinMaxScaler


train_data = pd.read_csv('california_housing_train.csv')
test_data = pd.read_csv('california_housing_train.csv') 


TARGET_NAME = 'median_house_value'
x_train, y_train = train_data.drop(TARGET_NAME, axis=1), train_data[TARGET_NAME]
x_test, y_test = test_data.drop(TARGET_NAME, axis=1), test_data[TARGET_NAME]


def scale_datasets(x_train, x_test):
    standard_scaler = MinMaxScaler()
    x_train_scaled = pd.DataFrame(
        standard_scaler.fit_transform(x_train),
        columns=x_train.columns
    )
    x_test_scaled = pd.DataFrame(
        standard_scaler.transform(x_test),
        columns=x_test.columns
    )
    return x_train_scaled, x_test_scaled


x_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)


# Define AutoEncoders class
class AutoEncoders(Model):
    def __init__(self, output_units):
        super(AutoEncoders, self).__init__()
        self.encoder = Sequential(
            [
                Dense(32, activation="relu"),
                Dense(16, activation="relu"),
                Dense(7, activation="relu")
            ]
        )
        
        self.decoder = Sequential(
            [
                Dense(16, activation="relu"),  # Corrected activation function
                Dense(32, activation="relu"),
                Dense(output_units, activation="sigmoid")
            ]
        )
    
    def call(self, inputs):
        encoded = self.encoder(inputs)
        decoded = self.decoder(encoded)
        return decoded


auto_encoder = AutoEncoders(len(x_train_scaled.columns))
auto_encoder.compile(
    loss='mae',
    metrics=['mae'],
    optimizer='adam'
)


# Train the model
history = auto_encoder.fit(
    x_train_scaled,
    x_train_scaled,
    epochs=15,
    batch_size=32,
    validation_data=(x_test_scaled, x_test_scaled)
)


# After training, create a separate model for the encoder
encoder_model = auto_encoder.encoder
reduced_df = pd.DataFrame(encoder_model.predict(x_train_scaled))
reduced_df = reduced_df.add_prefix('feature_')

# Display the reduced DataFrame with encoded features
print(reduced_df.head())
