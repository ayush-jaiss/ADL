import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.utils import to_categorical


(train_ds, train_labels), (test_ds, test_labels) =keras.datasets.cifar10.load_data()


train_ds=train_ds[:500]
train_labels=train_labels[:500]
test_ds=test_ds[:500]
test_labels=test_labels[:500]


train_ds[0].shape


train_labels = to_categorical(train_labels, num_classes=10) 
test_labels = to_categorical(test_labels, num_classes=10)


train_labels[0]


from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
train_ds[0].shape


base_model = VGG16(weights="imagenet", include_top=False, input_shape=train_ds[0].shape)


base_model.trainable = False
## Preprocessing input 
train_ds = preprocess_input(train_ds) 
test_ds = preprocess_input(test_ds)


## model details
base_model.summary()


#add our layers on top of this model 
from tensorflow.keras import layers, models

flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu') 
dense_layer_2 = layers.Dense (20, activation='relu') 
prediction_layer = layers. Dense(10, activation='softmax')

model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])


from tensorflow.keras.callbacks import EarlyStopping

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)


es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)


model.fit(train_ds, train_labels, epochs=5, validation_split=0.2, batch_size=32, callbacks=[es])
